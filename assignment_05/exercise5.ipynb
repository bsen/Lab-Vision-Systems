{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from my_utils import device, set_random_seed\n",
    "from my_torch_train import train_epoch, eval_model, train_model, count_model_params\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    " \n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting data loaders for iterating\n",
    "B_SIZE = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=B_SIZE, \n",
    "                                           shuffle=True) \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=B_SIZE,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general formulas behind one LSTM-cell are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "        i &= \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
    "        f &= \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
    "        g &= \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
    "        o &= \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
    "        c' &= f * c + i * g \\\\\n",
    "        h' &= o * \\tanh(c') \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.L_ii = nn.Linear(input_dim, hidden_dim).to(device)\n",
    "        self.L_hi = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        self.L_if = nn.Linear(input_dim, hidden_dim).to(device)\n",
    "        self.L_hf = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        self.L_ig = nn.Linear(input_dim, hidden_dim).to(device)\n",
    "        self.L_hg = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        self.L_io = nn.Linear(input_dim, hidden_dim).to(device)\n",
    "        self.L_ho = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "    \n",
    "    def forward(self, x, c, h):\n",
    "        i = torch.sigmoid(self.L_ii(x) + self.L_hi(h))\n",
    "        f = torch.sigmoid(self.L_if(x) + self.L_hf(h))\n",
    "        g = torch.tanh(self.L_ig(x) + self.L_hg(h))\n",
    "        o = torch.sigmoid(self.L_io(x) + self.L_ho(h))\n",
    "        c_out = f * c + i*g\n",
    "        h_out = o * torch.tanh(c_out)\n",
    "        return c_out, h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.lstm_cells=nn.ModuleList()\n",
    "        self.lstm_cells.append(myLSTMCell(input_dim, hidden_dim))\n",
    "        for i in range(num_layers-1):\n",
    "            self.lstm_cells.append(myLSTMCell(hidden_dim, hidden_dim))\n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        h = hc[0]\n",
    "        c = hc[1]\n",
    "        b_size, n_rows, n_cols = x.shape\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            new_x = torch.empty((b_size, n_rows, self.hidden_dim))\n",
    "            h_t = h[i]\n",
    "            c_t = c[i]\n",
    "\n",
    "            for i in range(n_rows):\n",
    "                c_t, h_t = self.lstm_cells[i](x[:,i,:], c_t, h_t)\n",
    "                if n_rows != num_layers-1:\n",
    "                    new_x[:, i, :] = h_t\n",
    "            x = new_x\n",
    "        \n",
    "        return h_t\n",
    "        #return h_list, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential classifier for images. Embedded image rows are fed to a RNN\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    input_dim: integer\n",
    "        dimensionality of the rows to embed\n",
    "    emb_dim: integer\n",
    "        dimensionality of the vectors fed to the LSTM\n",
    "    hidden_dim: integer\n",
    "        dimensionality of the states in the cell\n",
    "    use_own: boolean\n",
    "        When true uses our LSTM-implementation rather than the one from pytorch\n",
    "    init_mode: string\n",
    "        intialization of the states\n",
    "    num_layers: integer\n",
    "        the number of LSTM layers the classifier should use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, use_own=True, init_mode='zeros',\n",
    "                 num_layers=1):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        assert init_mode in [\"zeros\", \"random\", \"learned\"]\n",
    "        super().__init__()\n",
    "        self.hidden_dim =  hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.use_own = use_own\n",
    "\n",
    "        self.init_mode = init_mode\n",
    "        \n",
    "        # for embedding rows into vector representations\n",
    "        self.encoder = nn.Linear(in_features=input_dim, out_features=emb_dim).to(device)\n",
    "        \n",
    "        # lstm model\n",
    "        if self.use_own:\n",
    "            self.lstm = myLSTM(input_dim, hidden_dim, num_layers=num_layers)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=input_dim, hidden_size=hidden_dim, batch_first=True,\n",
    "                num_layers=num_layers\n",
    "            )\n",
    "        \n",
    "        # FC-classifier\n",
    "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=10)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through model \"\"\"\n",
    "\n",
    "        b_size, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device)\n",
    "\n",
    "        # embedding rows\n",
    "        x_rowed = x.view(b_size, n_channels*n_rows, n_cols)\n",
    "        embeddings = self.encoder(x_rowed)\n",
    "\n",
    "        # classifying\n",
    "        if self.use_own:\n",
    "            lstm_out = self.lstm(x_rowed, (h,c))\n",
    "            y = self.classifier(lstm_out)\n",
    "        else:\n",
    "            lstm_out, (h_out, c_out) = self.lstm(x_rowed, (h,c))\n",
    "            y = self.classifier(lstm_out[:, -1, :])  # feeding only output at last layer\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.init_mode == \"zeros\"):\n",
    "            h = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.init_mode == \"random\"):\n",
    "            h = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.init_mode == \"learned\"):\n",
    "            h = self.learned_h.repeat(1, b_size, 1)\n",
    "            c = self.learned_c.repeat(1, b_size, 1)\n",
    "        h = h.to(device)\n",
    "        c = c.to(device)\n",
    "        return h, c\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_lstm = SequentialClassifier(input_dim=28, emb_dim=64, hidden_dim=128, use_own=False, init_mode=\"zeros\", num_layers=2).to(device)\n",
    "own_lstm = SequentialClassifier(input_dim=28, emb_dim=64, hidden_dim=128, use_own=True, init_mode=\"zeros\", num_layers=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216138"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_params(torch_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216138"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_params(own_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model and the model using nn.LSTM have the same number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, save_path=None, num_epochs=10):\n",
    "    # classification loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer = torch.optim.Adam(torch_lstm.parameters(), lr=3e-4)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n",
    "\n",
    "    return train_model(\n",
    "            model=torch_lstm, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "            train_loader=train_loader, valid_loader=test_loader, num_epochs=num_epochs,\n",
    "            save_path=save_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, loss_iters, valid_acc = train_lstm(\n",
    "        model=torch_lstm, save_path='models/nnLSTM_1layer.pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using myLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, loss_iters, valid_acc = train_lstm(\n",
    "        model=own_lstm, save_path='models/ownLSTM_1layer.pt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

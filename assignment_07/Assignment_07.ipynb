{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8273bd75-f865-46da-aea0-5918a4ee45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0eed180f-82e9-4c60-88ad-4fd58c6e715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size, num_blocks):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(3, img_size, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ]\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(\n",
    "                self._block(img_size*(2**i), img_size*(2**(i+1)), 4, 2, 1)\n",
    "            )\n",
    "        layers.extend([\n",
    "            nn.Conv2d(img_size*(2**(i+1)), 1, 4, 2, 0),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8b4701d-80be-4793-bcac-190a2bd47c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_size, num_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = [\n",
    "            self._block(z_dim, img_size*(2**num_blocks), 4, 1, 0)\n",
    "        ]\n",
    "        for i in range(num_blocks, 1, -1):\n",
    "            layers.append(\n",
    "                self._block(img_size*(2**i), img_size*(2**(i-1)), 4, 2, 1)\n",
    "            )\n",
    "        layers.extend([\n",
    "            nn.ConvTranspose2d(img_size*2, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82107433-4cb4-460a-9719-46dded41770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a6a3bf32-970c-404e-bd81-4934fa238e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    N = 8\n",
    "    z_dim = 100\n",
    "    x = torch.randn((N, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    disc = Discriminator(IMAGE_SIZE, 3)\n",
    "    initialize_weights(disc)\n",
    "    assert disc(x).shape == (N, 1, 1, 1)\n",
    "    \n",
    "    gen = Generator(z_dim, IMAGE_SIZE, 4)\n",
    "    initialize_weights(gen)\n",
    "    z = torch.randn((N, z_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8cebba81-3b12-45b4-86e8-9772e0bca557",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 2e-4\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88940cce-bcff-4992-9eec-03ac0feeb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(img_size, dataloader, num_blocks_gen, num_blocks_disc):\n",
    "    if os.path.exists(f'logs/{img_size}'):\n",
    "        shutil.rmtree(f'logs/{img_size}')\n",
    "    step = 0\n",
    "    writer = SummaryWriter(f'logs/{img_size}')\n",
    "    writer_real = SummaryWriter(f'logs/{img_size}/real')\n",
    "    writer_fake = SummaryWriter(f'logs/{img_size}/fake')\n",
    "\n",
    "    gen = Generator(Z_DIM, img_size, num_blocks_gen).to(device)\n",
    "    initialize_weights(gen)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "    disc = Discriminator(img_size, num_blocks_disc).to(device)\n",
    "    initialize_weights(disc)\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    fixed_noise = torch.randn(16, Z_DIM, 1, 1).to(device)\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for batch_idx, (real, _) in enumerate(dataloader):\n",
    "            real = real.to(device)\n",
    "            noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
    "            fake = gen(noise)\n",
    "\n",
    "            # train discriminator\n",
    "            disc_real = disc(real).reshape(-1)\n",
    "            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            disc_fake = disc(fake.detach()).reshape(-1)\n",
    "            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "            disc.zero_grad()\n",
    "            loss_disc.backward()\n",
    "            opt_disc.step()\n",
    "\n",
    "            # train generator\n",
    "            output = disc(fake).reshape(-1)\n",
    "            loss_gen = criterion(output, torch.ones_like(output))\n",
    "            gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "\n",
    "            writer.add_scalar(f'Loss/Discriminator Loss', loss_disc.item(), global_step=step)\n",
    "            writer.add_scalar(f'Loss/Generator Loss', loss_gen.item(), global_step=step)\n",
    "            writer.add_scalars(f'Comb_Loss/Losses', {\n",
    "                'Discriminator': loss_disc.item(),\n",
    "                'Generator':  loss_gen.item()\n",
    "            }, step)    \n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(trainloader)}\\nLossD: {loss_disc:.4f}, LossG: {loss_gen:.4f}\\n\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    fake = gen(fixed_noise)\n",
    "\n",
    "                    img_grid_real = torchvision.utils.make_grid(real[:16], normalize=True)\n",
    "                    img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "\n",
    "                    writer_real.add_image('Real', img_grid_real, global_step=step)\n",
    "                    writer_fake.add_image('Fake', img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "68b0317b-1aea-4721-86b7-bcf2dacd35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader_32 = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9b77f3c3-2618-4116-acc1-28d56a1df688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/391\n",
      "LossD: 0.6910, LossG: 0.7095\n",
      "\n",
      "Epoch [0/10] Batch 100/391\n",
      "LossD: 0.1574, LossG: 2.0112\n",
      "\n",
      "Epoch [0/10] Batch 200/391\n",
      "LossD: 0.0658, LossG: 3.1542\n",
      "\n",
      "Epoch [0/10] Batch 300/391\n",
      "LossD: 0.3034, LossG: 2.1225\n",
      "\n",
      "Epoch [1/10] Batch 0/391\n",
      "LossD: 0.4970, LossG: 1.8705\n",
      "\n",
      "Epoch [1/10] Batch 100/391\n",
      "LossD: 0.5904, LossG: 1.1150\n",
      "\n",
      "Epoch [1/10] Batch 200/391\n",
      "LossD: 0.4490, LossG: 1.4704\n",
      "\n",
      "Epoch [1/10] Batch 300/391\n",
      "LossD: 0.6010, LossG: 1.6019\n",
      "\n",
      "Epoch [2/10] Batch 0/391\n",
      "LossD: 0.5286, LossG: 1.2366\n",
      "\n",
      "Epoch [2/10] Batch 100/391\n",
      "LossD: 0.5254, LossG: 1.3205\n",
      "\n",
      "Epoch [2/10] Batch 200/391\n",
      "LossD: 0.5388, LossG: 1.2472\n",
      "\n",
      "Epoch [2/10] Batch 300/391\n",
      "LossD: 0.5082, LossG: 1.2911\n",
      "\n",
      "Epoch [3/10] Batch 0/391\n",
      "LossD: 0.4895, LossG: 1.2482\n",
      "\n",
      "Epoch [3/10] Batch 100/391\n",
      "LossD: 0.4425, LossG: 1.4490\n",
      "\n",
      "Epoch [3/10] Batch 200/391\n",
      "LossD: 0.4804, LossG: 1.4477\n",
      "\n",
      "Epoch [3/10] Batch 300/391\n",
      "LossD: 0.3768, LossG: 1.7165\n",
      "\n",
      "Epoch [4/10] Batch 0/391\n",
      "LossD: 0.4379, LossG: 1.6753\n",
      "\n",
      "Epoch [4/10] Batch 100/391\n",
      "LossD: 0.4921, LossG: 1.8404\n",
      "\n",
      "Epoch [4/10] Batch 200/391\n",
      "LossD: 0.5019, LossG: 1.8365\n",
      "\n",
      "Epoch [4/10] Batch 300/391\n",
      "LossD: 0.4697, LossG: 1.9009\n",
      "\n",
      "Epoch [5/10] Batch 0/391\n",
      "LossD: 0.4632, LossG: 1.4773\n",
      "\n",
      "Epoch [5/10] Batch 100/391\n",
      "LossD: 0.5231, LossG: 1.2332\n",
      "\n",
      "Epoch [5/10] Batch 200/391\n",
      "LossD: 0.5609, LossG: 1.1890\n",
      "\n",
      "Epoch [5/10] Batch 300/391\n",
      "LossD: 0.4867, LossG: 1.2176\n",
      "\n",
      "Epoch [6/10] Batch 0/391\n",
      "LossD: 0.5087, LossG: 1.2415\n",
      "\n",
      "Epoch [6/10] Batch 100/391\n",
      "LossD: 0.5619, LossG: 1.2659\n",
      "\n",
      "Epoch [6/10] Batch 200/391\n",
      "LossD: 0.4859, LossG: 1.4258\n",
      "\n",
      "Epoch [6/10] Batch 300/391\n",
      "LossD: 0.5008, LossG: 1.6213\n",
      "\n",
      "Epoch [7/10] Batch 0/391\n",
      "LossD: 0.5096, LossG: 0.9821\n",
      "\n",
      "Epoch [7/10] Batch 100/391\n",
      "LossD: 0.5280, LossG: 1.0471\n",
      "\n",
      "Epoch [7/10] Batch 200/391\n",
      "LossD: 0.4967, LossG: 1.2588\n",
      "\n",
      "Epoch [7/10] Batch 300/391\n",
      "LossD: 0.5281, LossG: 1.2442\n",
      "\n",
      "Epoch [8/10] Batch 0/391\n",
      "LossD: 0.5251, LossG: 1.4365\n",
      "\n",
      "Epoch [8/10] Batch 100/391\n",
      "LossD: 0.5114, LossG: 1.1081\n",
      "\n",
      "Epoch [8/10] Batch 200/391\n",
      "LossD: 0.4673, LossG: 1.1428\n",
      "\n",
      "Epoch [8/10] Batch 300/391\n",
      "LossD: 0.4928, LossG: 1.3018\n",
      "\n",
      "Epoch [9/10] Batch 0/391\n",
      "LossD: 1.0697, LossG: 2.4265\n",
      "\n",
      "Epoch [9/10] Batch 100/391\n",
      "LossD: 0.4423, LossG: 1.4751\n",
      "\n",
      "Epoch [9/10] Batch 200/391\n",
      "LossD: 0.4432, LossG: 1.1698\n",
      "\n",
      "Epoch [9/10] Batch 300/391\n",
      "LossD: 0.5289, LossG: 1.2089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(IMAGE_SIZE, trainloader_32, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "df25750c-5366-4a65-b67b-68f994e2f04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader_64 = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec50258d-c2b2-4525-a893-fa4d4cdd1a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/391\n",
      "LossD: 0.6881, LossG: 0.8069\n",
      "\n",
      "Epoch [0/10] Batch 100/391\n",
      "LossD: 0.0325, LossG: 3.6405\n",
      "\n",
      "Epoch [0/10] Batch 200/391\n",
      "LossD: 0.2608, LossG: 2.3286\n",
      "\n",
      "Epoch [0/10] Batch 300/391\n",
      "LossD: 0.4734, LossG: 1.4808\n",
      "\n",
      "Epoch [1/10] Batch 0/391\n",
      "LossD: 0.4822, LossG: 1.5392\n",
      "\n",
      "Epoch [1/10] Batch 100/391\n",
      "LossD: 0.7259, LossG: 0.8036\n",
      "\n",
      "Epoch [1/10] Batch 200/391\n",
      "LossD: 0.5151, LossG: 1.6969\n",
      "\n",
      "Epoch [1/10] Batch 300/391\n",
      "LossD: 0.5888, LossG: 1.5069\n",
      "\n",
      "Epoch [2/10] Batch 0/391\n",
      "LossD: 0.5908, LossG: 1.1664\n",
      "\n",
      "Epoch [2/10] Batch 100/391\n",
      "LossD: 0.6890, LossG: 1.4918\n",
      "\n",
      "Epoch [2/10] Batch 200/391\n",
      "LossD: 0.5097, LossG: 1.5576\n",
      "\n",
      "Epoch [2/10] Batch 300/391\n",
      "LossD: 0.5803, LossG: 1.5948\n",
      "\n",
      "Epoch [3/10] Batch 0/391\n",
      "LossD: 0.6765, LossG: 1.1348\n",
      "\n",
      "Epoch [3/10] Batch 100/391\n",
      "LossD: 0.5655, LossG: 1.2603\n",
      "\n",
      "Epoch [3/10] Batch 200/391\n",
      "LossD: 0.5615, LossG: 1.0650\n",
      "\n",
      "Epoch [3/10] Batch 300/391\n",
      "LossD: 0.6415, LossG: 0.8331\n",
      "\n",
      "Epoch [4/10] Batch 0/391\n",
      "LossD: 0.5932, LossG: 1.7194\n",
      "\n",
      "Epoch [4/10] Batch 100/391\n",
      "LossD: 0.5089, LossG: 1.4270\n",
      "\n",
      "Epoch [4/10] Batch 200/391\n",
      "LossD: 0.6333, LossG: 1.4516\n",
      "\n",
      "Epoch [4/10] Batch 300/391\n",
      "LossD: 0.5629, LossG: 1.8328\n",
      "\n",
      "Epoch [5/10] Batch 0/391\n",
      "LossD: 0.3724, LossG: 1.7438\n",
      "\n",
      "Epoch [5/10] Batch 100/391\n",
      "LossD: 0.5933, LossG: 1.8841\n",
      "\n",
      "Epoch [5/10] Batch 200/391\n",
      "LossD: 0.4493, LossG: 1.6534\n",
      "\n",
      "Epoch [5/10] Batch 300/391\n",
      "LossD: 0.4509, LossG: 1.6239\n",
      "\n",
      "Epoch [6/10] Batch 0/391\n",
      "LossD: 0.4870, LossG: 0.9548\n",
      "\n",
      "Epoch [6/10] Batch 100/391\n",
      "LossD: 0.4577, LossG: 1.6443\n",
      "\n",
      "Epoch [6/10] Batch 200/391\n",
      "LossD: 0.3086, LossG: 1.8713\n",
      "\n",
      "Epoch [6/10] Batch 300/391\n",
      "LossD: 0.4926, LossG: 1.0502\n",
      "\n",
      "Epoch [7/10] Batch 0/391\n",
      "LossD: 1.1742, LossG: 0.6725\n",
      "\n",
      "Epoch [7/10] Batch 100/391\n",
      "LossD: 0.4287, LossG: 1.4273\n",
      "\n",
      "Epoch [7/10] Batch 200/391\n",
      "LossD: 0.1326, LossG: 3.9143\n",
      "\n",
      "Epoch [7/10] Batch 300/391\n",
      "LossD: 0.4099, LossG: 1.8873\n",
      "\n",
      "Epoch [8/10] Batch 0/391\n",
      "LossD: 0.1165, LossG: 3.0919\n",
      "\n",
      "Epoch [8/10] Batch 100/391\n",
      "LossD: 0.0704, LossG: 2.9165\n",
      "\n",
      "Epoch [8/10] Batch 200/391\n",
      "LossD: 0.3362, LossG: 1.4244\n",
      "\n",
      "Epoch [8/10] Batch 300/391\n",
      "LossD: 0.1501, LossG: 2.2720\n",
      "\n",
      "Epoch [9/10] Batch 0/391\n",
      "LossD: 0.3429, LossG: 2.2738\n",
      "\n",
      "Epoch [9/10] Batch 100/391\n",
      "LossD: 0.0562, LossG: 3.8305\n",
      "\n",
      "Epoch [9/10] Batch 200/391\n",
      "LossD: 0.0448, LossG: 4.0236\n",
      "\n",
      "Epoch [9/10] Batch 300/391\n",
      "LossD: 0.3597, LossG: 0.1057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(IMAGE_SIZE, trainloader_64, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c1d1debc-e544-4ccc-b1d4-8e39894d12e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discriminator(img_size=64, num_blocks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c349d01d-5376-4820-9ecb-9feaf3dff755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(z_dim=100, img_size=64, num_blocks=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "43f3a49a-fe34-4abd-bc44-f23527630420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discriminator(img_size=32, num_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "69eb969a-23cc-4b14-8038-621d0789655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(z_dim=100, img_size=32, num_blocks=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
